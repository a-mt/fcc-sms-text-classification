{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries\n",
    "\n",
    "https://www.tensorflow.org/lite/guide/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle as pickle\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tflite-runtime==2.5.0 from https://github.com/google-coral/pycoral/releases/download/release-frogfish/tflite_runtime-2.5.0-cp37-cp37m-linux_x86_64.whl\n",
      "\u001b[?25l  Downloading https://github.com/google-coral/pycoral/releases/download/release-frogfish/tflite_runtime-2.5.0-cp37-cp37m-linux_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 449kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.1 in /home/myself/anaconda3/lib/python3.7/site-packages (from tflite-runtime==2.5.0) (1.18.5)\n",
      "Installing collected packages: tflite-runtime\n",
      "Successfully installed tflite-runtime-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/google-coral/pycoral/releases/download/release-frogfish/tflite_runtime-2.5.0-cp37-cp37m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflite_runtime.interpreter import Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras_preprocessing in /home/myself/anaconda3/lib/python3.7/site-packages (1.1.2)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /home/myself/anaconda3/lib/python3.7/site-packages (from keras_preprocessing) (1.15.0)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/myself/anaconda3/lib/python3.7/site-packages (from keras_preprocessing) (1.18.5)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install keras_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/f6/72db36e8afc977ae1a1cbb22afc77fd9b514e9bc6927ae8f4aae36665961/wn-0.0.23.tar.gz (31.6MB)\n",
      "\u001b[K     |████████████████████████████████| 31.6MB 7.9MB/s eta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: wn\n",
      "  Building wheel for wn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/myself/.cache/pip/wheels/56/e3/c4/886021dbf4d758dc3cb9ddaa47d7d6fc895240d83f010e6305\n",
      "Successfully built wn\n",
      "Installing collected packages: wn\n",
      "Successfully installed wn-0.0.23\n"
     ]
    }
   ],
   "source": [
    "!pip install wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using standalone wordnet instead of nltk:\n",
    "\n",
    "def lemmatize(self, word, pos=NOUN):\n",
    "    lemmas = wordnet._morphy(word, pos)\n",
    "    return min(lemmas, key=len) if lemmas else word\n",
    "'''\n",
    "from wn.morphy import _morphy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"resources/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "interpreter = Interpreter(\n",
    "    model_path=path + 'fcc_sms_classification.tflite'\n",
    ")\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class objectview(object):\n",
    "    \"\"\"\n",
    "    Trick to access dictionary items as object attributes\n",
    "    \"\"\"\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d\n",
    "\n",
    "with open(path + \"utils.pkl\", \"rb\") as f:\n",
    "    utils = objectview(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesssing txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abaci'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize(word, pos='n'):\n",
    "    '''\n",
    "    Parts of speech constants:\n",
    "    ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "    '''\n",
    "    lemmas = _morphy(word, pos)\n",
    "    return min(lemmas, key=len) if lemmas else word\n",
    "\n",
    "lemmatize('abaci')\n",
    "\n",
    "# https://pypi.org/pypi/wn/json\n",
    "# print(inspect.getsource(WordNetLemmatizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(txt):\n",
    "    txt = re.sub(r'([^\\s\\w])+', ' ', txt)\n",
    "    txt = \" \".join([lemmatize(word) for word in txt.split()\n",
    "                    if not word in utils.stopwords_eng])\n",
    "    txt = txt.lower()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ahhhh woken bad dream u tho dont like u right didnt know anything comedy night guess im'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = 'ahhhh...just woken up!had a bad dream about u tho,so i dont like u right now :) i didnt know anything about comedy night but i guess im up for it.'\n",
    "cleanup(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[309, 227, 1, 587, 42, 15, 1, 90, 359, 13, 103, 54, 228, 86]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.texts_to_sequences([cleanup(txt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13, 103,  54, 228,  86], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 500\n",
    "\n",
    "def preprocessing(X):\n",
    "    return utils.pad_sequences(\n",
    "        utils.texts_to_sequences([cleanup(x) for x in X]),\n",
    "        maxlen=max_len)\n",
    "\n",
    "preprocessing([txt])[0][-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "https://medium.com/@mmohamedrashik/how-to-deploy-tensorflow-regression-model-in-android-tf-lite-part-2-90b9ebb31903"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    input_index  = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    input_data = np.array(X, dtype=np.float32)\n",
    "    interpreter.set_tensor(input_index, input_data)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    return interpreter.get_tensor(output_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3542966]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(preprocessing([\n",
    "    \"you have won £1000 cash! call to claim your prize.\"\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set FastAPI settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/0b/5df17eaadb7fe39dad349f484e551e802ce0581be672822f010c530d5e75/fastapi-0.61.2-py3-none-any.whl (48kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 2.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting starlette==0.13.6 (from fastapi)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/a4/c9e228d7d47044ce4c83ba002f28ff479e542455f0499198a3f77c94f564/starlette-0.13.6-py3-none-any.whl (59kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 6.5MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pydantic<2.0.0,>=1.0.0 (from fastapi)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/a8/01a6ebf62e7234deffc747d161dacdc29255382610df40f6293ca58fd4fd/pydantic-1.7.2-py3-none-any.whl (107kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 8.4MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: starlette, pydantic, fastapi\n",
      "Successfully installed fastapi-0.61.2 pydantic-1.7.2 starlette-0.13.6\n"
     ]
    }
   ],
   "source": [
    "!pip install fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting uvicorn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/cc/01cc4cb980dfcf04eb283b6497c7f280928a0b02c68c0f85b6901e7716ae/uvicorn-0.12.2-py3-none-any.whl (45kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 2.6MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: h11>=0.8 in /home/myself/anaconda3/lib/python3.7/site-packages (from uvicorn) (0.9.0)\n",
      "Requirement already satisfied: click==7.* in /home/myself/anaconda3/lib/python3.7/site-packages (from uvicorn) (7.0)\n",
      "Collecting typing-extensions; python_version < \"3.8\" (from uvicorn)\n",
      "  Downloading https://files.pythonhosted.org/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl\n",
      "Installing collected packages: typing-extensions, uvicorn\n",
      "Successfully installed typing-extensions-3.7.4.3 uvicorn-0.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Pydantic BaseModel class for automatic data validation\n",
    "class Data(BaseModel):\n",
    "    text: str\n",
    "\n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"text\": \"you have won £1000 cash! call to claim your prize.\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Defining Response format for documentation\n",
    "class Response(BaseModel):\n",
    "    p: float\n",
    "    prediction: str\n",
    "\n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"prediction\": \"ham\",\n",
    "                \"p\": 0\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI(debug=True)\n",
    "\n",
    "@app.post(\"/predict\", response_model=Response)\n",
    "def app_predict(data: Data):\n",
    "    try:\n",
    "        text = data.text\n",
    "        res  = predict(preprocessing([text]))[0]\n",
    "        p    = float(res[0])\n",
    "\n",
    "        return {\"prediction\": (\"ham\" if p<0.5 else \"spam\"), \"p\": p}\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {\"prediction\" : \"error\"}\n",
    "    \n",
    "@app.get('/healthcheck', status_code=200)\n",
    "async def healthcheck():\n",
    "    return 'OK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a .py file\n",
    "\n",
    "    ```\n",
    "    ipython nbconvert FastAPI.ipynb --to script\n",
    "    ```\n",
    "\n",
    "* Edit the .py file to remove unnecessary steps — ie function checking, installs\n",
    "\n",
    "* Start the server\n",
    "\n",
    "    ```\n",
    "    uvicorn FastAPI:app --host=127.0.0.1 --port=${PORT:-8000}\n",
    "    ```\n",
    "\n",
    "* Go to http://127.0.0.1:8000/docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative way to Start server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.0.0.1 : 8000\n"
     ]
    }
   ],
   "source": [
    "from os import environ\n",
    "\n",
    "HOST = environ.get('HOST', \"127.0.0.1\")\n",
    "PORT = int(environ.get('PORT', 8000))\n",
    "\n",
    "print(HOST, ':', PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    uvicorn.run(app, host=HOST, port=PORT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Start the server\n",
    "\n",
    "    ```\n",
    "    python FastAPI.py\n",
    "    ```\n",
    "\n",
    "## Deploy on Heroku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create `runtime.txt` to specify the Python version to use. Make sure it is a [supported runtime](https://devcenter.heroku.com/articles/python-support#supported-runtimes)\n",
    "\n",
    "    ```\n",
    "    python-3.7.9\n",
    "    ```\n",
    "\n",
    "* Create `Procfile`\n",
    "\n",
    "    ```\n",
    "    web: uvicorn FastAPI:app --host=0.0.0.0 --port=${PORT:-5000}\n",
    "    ```\n",
    "\n",
    "* Create `requirements.txt`\n",
    "\n",
    "    ```\n",
    "    https://github.com/google-coral/pycoral/releases/download/release-frogfish/tflite_runtime-2.5.0-cp37-cp37m-linux_x86_64.whl\n",
    "    fastapi\n",
    "    uvicorn\n",
    "    pydantic\n",
    "    cloudpickle\n",
    "    keras_preprocessing\n",
    "    wn\n",
    "    ```\n",
    "\n",
    "* Commit your files.\n",
    "\n",
    "    ```\n",
    "    git init\n",
    "    git add runtime.txt Procfile requirements.txt FastAPI.py models/\n",
    "    git commit -m \"Initial Deployment\"\n",
    "    ```\n",
    "\n",
    "* Create a Heroku app\n",
    "\n",
    "    ```\n",
    "    heroku login\n",
    "    heroku create\n",
    "    git push heroku master\n",
    "    ```\n",
    "\n",
    "If you want: [Set up auto-deploy](https://towardsdatascience.com/autodeploy-fastapi-app-to-heroku-via-git-in-these-5-easy-steps-8c7958ef5d41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport requests\\n\\nurl = 'http://127.0.0.1:8000/predict'\\n\\nr = requests.post(url, json={\\n    'text': 'you have won £1000 cash! call to claim your prize.'\\n})\\nr.json()\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import requests\n",
    "\n",
    "url = 'http://127.0.0.1:8000/predict'\n",
    "\n",
    "r = requests.post(url, json={\n",
    "    'text': 'you have won £1000 cash! call to claim your prize.'\n",
    "})\n",
    "r.json()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
